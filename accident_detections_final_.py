# -*- coding: utf-8 -*-
"""Accident_Detections_Final .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-TdfAXMwaFEKVhLHpQRGDCC77xf_X4ck
"""

from google.colab import drive
drive.mount('/content/drive')

pip install pydotplus

pip install keras-tuner

#Importing the Libraries

import pandas as pd
import numpy as np
import seaborn as sns
from collections import Counter
import matplotlib.pyplot as plt
import plotly
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder,MinMaxScaler,RobustScaler,FunctionTransformer
from sklearn.datasets import load_iris,load_diabetes
from sklearn.model_selection import train_test_split,GridSearchCV
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns
import pickle 
import keras
from keras.models import Sequential
from keras.layers import Dense,Dropout,BatchNormalization
from keras.optimizers import Adam
import keras_tuner as kt
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler,FunctionTransformer,PowerTransformer
import tensorflow
from sklearn.impute import KNNImputer
import missingno
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense,Flatten
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
import graphviz
import pydotplus

from sklearn import set_config
set_config(display='diagram')

pip install tensorflow

pip install keras



import warnings
warnings.filterwarnings('ignore')

data=pd.read_csv('/content/drive/MyDrive/major_project_data.csv')

print(data.shape)

"""### Dropping the Duplicate rows(if it exists from the dataframe)"""

data=data.drop_duplicates()

print(data.info())

def conitnuous_columns_univariate(column_name):
    '''
    Univariate Analysis for numerical column
    '''
    print(f'Column {column_name} has following description : ')
    print()
    print(data[column_name].describe())
    print()
    fig,axs=plt.subplots(ncols=2,figsize=(12,4))
    hist=sns.histplot(data=data,x=column_name,kde=True,ax=axs[0])
    axs[0].set_title(f"Histogram for {column_name}")
    sns.boxplot(data=data,x=column_name,ax=axs[1])
    axs[1].set_title(f"Boxplot for {column_name}")
    print()
    print(f'{column_name} has skew={data[column_name].skew()}')
    print()
    plt.show()

conitnuous_columns_univariate('Start_Lat')

conitnuous_columns_univariate('Start_Lng')

conitnuous_columns_univariate('End_Lat')

conitnuous_columns_univariate('End_Lng')

conitnuous_columns_univariate('Distance(mi)')

conitnuous_columns_univariate('Number')

conitnuous_columns_univariate('Temperature(F)')

conitnuous_columns_univariate('Wind_Chill(F)')

conitnuous_columns_univariate('Humidity(%)')

conitnuous_columns_univariate('Pressure(in)')

conitnuous_columns_univariate('Visibility(mi)')

conitnuous_columns_univariate('Precipitation(in)')

conitnuous_columns_univariate('Wind_Speed(mph)')

def categorical_columns_univariate(column_name):
  '''
  Univariate analysis for categorical columns
  '''
    fig,axs=plt.subplots(ncols=2,figsize=(10,6))
    data[column_name].value_counts().plot(kind='bar',ax=axs[0])
    axs[0].set_title(f"Bar plot for {column_name}")
    data[column_name].value_counts().plot(kind='pie',autopct='%0.1f%%',ax=axs[1])
    axs[1].set_title(f"Pie chart for {column_name}")
    plt.tight_layout()
    plt.show()

categorical_columns_univariate('Amenity')

categorical_columns_univariate('Bump')

del data['Bump']

categorical_columns_univariate('Crossing')

categorical_columns_univariate('Give_Way')

categorical_columns_univariate('Junction')

categorical_columns_univariate('No_Exit')

categorical_columns_univariate('Railway')

categorical_columns_univariate('Roundabout')

del data['Roundabout']

categorical_columns_univariate('Station')

categorical_columns_univariate('Stop')

categorical_columns_univariate('Traffic_Calming')

categorical_columns_univariate('Traffic_Signal')

categorical_columns_univariate('Turning_Loop')

del data['Turning_Loop']

categorical_columns_univariate('Country')

del data['Country']

categorical_columns_univariate('State')

sampled_data=data.sample(n=1000,random_state=42)

print(dict(Counter(sampled_data['Street'])))

print(dict(Counter(sampled_data['Side'])))

categorical_columns_univariate('Side')

print(dict(Counter(sampled_data['City'])))

print(dict(Counter(sampled_data['County'])))

print(dict(Counter(sampled_data['Zipcode'])))

print(dict(Counter(sampled_data['Timezone'])))

categorical_columns_univariate('Timezone')

print(dict(Counter(sampled_data['Airport_Code'])))

print(dict(Counter(sampled_data['Weather_Timestamp'])))

print(dict(Counter(sampled_data['Wind_Direction'])))

categorical_columns_univariate('Wind_Direction')

print(dict(Counter(sampled_data['Weather_Condition'])))

categorical_columns_univariate('Weather_Condition')

print(dict(Counter(sampled_data['Sunrise_Sunset'])))

categorical_columns_univariate('Sunrise_Sunset')

print(dict(Counter(sampled_data['Civil_Twilight'])))

categorical_columns_univariate('Civil_Twilight')

categorical_columns_univariate('Nautical_Twilight')

categorical_columns_univariate('Astronomical_Twilight')

#Bivariate
def categorical_categorical(column_1='Severity',column_2=None):
    ctab=pd.crosstab(data[column_1],data[column_2],normalize='columns')
    fig,axs=plt.subplots(nrows=1,ncols=2,figsize=(10,5))
    sns.heatmap(ctab,annot=True,cmap='Blues',ax=axs[0])
    axs[0].set_title(f'Heatmap of {column_1} v/s {column_2}')
    ctab.plot(kind='bar', stacked=True, ax=axs[1])
    axs[1].set_xlabel(column_1)
    axs[1].set_ylabel('Count')
    axs[1].set_title(f'Stacked Barplot of {column_1} v/s {column_2}')
    plt.subplots_adjust(wspace=0.3)
    plt.show()

categorical_categorical(column_2='Side')

categorical_categorical(column_2='Timezone')

categorical_categorical(column_2='Sunrise_Sunset')

categorical_categorical(column_2='Civil_Twilight')

categorical_categorical(column_2='Nautical_Twilight')

categorical_categorical(column_2='Astronomical_Twilight')

categorical_categorical(column_2='Amenity')

categorical_categorical(column_2='Crossing')

categorical_categorical(column_2='Give_Way')

categorical_categorical(column_2='Junction')

categorical_categorical(column_2='No_Exit')

categorical_categorical(column_2='Railway')

categorical_categorical(column_2='Station')

categorical_categorical(column_2='Stop')

categorical_categorical(column_2='Traffic_Calming')

categorical_categorical(column_2='Traffic_Signal')

def correlation_heatmap():
    corr=data.corr()
    fig,ax = plt.subplots(figsize=(20,20))
    sns.heatmap(corr,cmap='coolwarm', vmax=1, vmin=-1, center=0,square=True,linewidths=.5,cbar_kws={"shrink": .5},annot=True)
    ax.set_title('Correlation Heatmap')
    plt.show()

correlation_heatmap()

def delete_low_corr_cols(data):
    corr_matrix=data.corr()
    severity_corr=corr_matrix['Severity']
    low_corr_cols=severity_corr[abs(severity_corr)<0.01].index.tolist()
    data=data.drop(columns=low_corr_cols)
    print(low_corr_cols)
    return data

data=delete_low_corr_cols(data)

correlation_heatmap()

del data['Wind_Chill(F)']
del data['Start_Lat']
del data['Start_Lng']

correlation_heatmap()

null_columns=list(data.columns[data.isnull().any()])
missingno.matrix(data[null_columns],figsize=(10, 6));

state_counts=data["State"].value_counts()
fig=plotly.graph_objects.Figure(data=plotly.graph_objects.Choropleth(locations=state_counts.index,z=state_counts.values.astype(float),locationmode="USA-states",colorscale="turbo"))
fig.update_layout(geo_scope="usa")
fig.show()

fig,ax=plt.subplots(figsize=(25,7))
sns.countplot(x="City",data=data,order=data.City.value_counts().iloc[:25].index)
plt.title('Top 25 Cities having Accidents in USA',fontsize=50)
plt.show()

for severity in set(data['Severity']):
    severity_data=data[data['Severity']==severity]
    fig,ax=plt.subplots(figsize=(25,7))
    sns.countplot(x="City",data=severity_data,order=severity_data.City.value_counts().iloc[:25].index)
    plt.title(f'Top 25 Cities having Accidents(Severity = {severity} ) in USA',fontsize=50)
    plt.show()

data.End_Time=pd.to_datetime(data.End_Time)
data['Month']=data['End_Time'].dt.month
data['Year']=data['End_Time'].dt.year
data['Hour']=data['End_Time'].dt.hour
data['Weekday']=data['End_Time'].dt.weekday

sns.countplot(x="Month",data=data)
plt.title('Monthwise Accidents')
plt.show()

print(set(data['Year']))

for year in set(data['Year']):
    if(year!=2022):
        yearwise_data=data[data['Year']==year]
        sns.countplot(x="Month",data=yearwise_data)
        plt.title(f'Monthwise Accidents for Year {year}')
        plt.show()

sns.countplot(x='Year',data=data)
plt.title('Yearwise Accident')
plt.show()

for severity in set(data['Severity']):
    severity_data=data[data['Severity']==severity]
    sns.countplot(x='Year',data=severity_data)
    plt.title(f'Yearwise Accident (Severity = {severity})')
    plt.show()

sns.countplot(x='Weekday',data=data)
plt.title('Accident Weekday wise')
plt.show()

for year in set(data['Year']):
    if(year!=2022):
        yearwise_data=data[data['Year']==year]
        sns.countplot(x="Weekday",data=yearwise_data)
        plt.title(f'Weekday Accidents for Year {year}')
        plt.show()

sns.countplot(x='Hour',data=data)
plt.title('Accident Hourly Basis')
plt.show()

for severity in set(data['Severity']):
    severity_data=data[data['Severity']==severity]
    sns.countplot(x='Hour',data=severity_data)
    plt.title(f'Hourly Accident (Severity = {severity})')
    plt.show()

for year in set(data['Year']):
    if(year!=2022):
        yearwise_data=data[data['Year']==year]
        sns.countplot(x="Hour",data=yearwise_data)
        plt.title(f'Hourly Accidents for Year {year}')
        plt.show()

for year in set(data['Year']):
    if(year!=2022):
        fig,ax=plt.subplots(figsize=(25,7))
        yearwise_data=data[data['Year']==year]
        sns.countplot(x=yearwise_data.End_Time.dt.isocalendar().week,data=yearwise_data)
        plt.title(f'Weekwise Accidents for Year {year}')
        plt.show()

for severity in set(data['Severity']):
    fig,ax=plt.subplots(figsize=(22,7))
    severity_data=data[data['Severity']==severity]
    sns.countplot(x='State',data=severity_data)
    plt.title(f'Accident Count in different States for Severity = {severity}',fontsize=20)
    plt.show()

top_cities = data['City'].value_counts().head(10)
sns.set_style("whitegrid")
plt.figure(figsize=(10, 6))
sns.barplot(x=top_cities.values, y=top_cities.index, color='royalblue')
plt.title('Top 10 Cities with the Maximum Accidents', fontsize=16)
plt.xlabel('Number of Accidents', fontsize=12)
plt.ylabel('City', fontsize=12)
plt.show()

weather_counts = data['Weather_Condition'].value_counts()
sorted_weather_counts = weather_counts.sort_values(ascending=False)
top_weather_conditions = sorted_weather_counts.head(20)
sns.set_style("whitegrid")
plt.figure(figsize=(12, 8))
sns.barplot(x=top_weather_conditions.values,y=top_weather_conditions.index,color='royalblue')
plt.title('Top 20 Weather Conditions with the Highest Number of Accidents', fontsize=16)
plt.xlabel('Number of Accidents', fontsize=12)
plt.ylabel('Weather Condition', fontsize=12)
plt.show()

weather_counts = data['Wind_Direction'].value_counts()
sorted_weather_counts = weather_counts.sort_values(ascending=False)
top_weather_conditions = sorted_weather_counts.head(20)
sns.set_style("whitegrid")
plt.figure(figsize=(12, 8))
sns.barplot(x=top_weather_conditions.values,y=top_weather_conditions.index,color='royalblue')
plt.title('Top 20 Wind Directions with the Highest Number of Accidents', fontsize=16)
plt.xlabel('Number of Accidents', fontsize=12)
plt.ylabel('Wind Directions', fontsize=12)
plt.show()

print(data.info())

def continuous_categorical(column_name):
    data[data['Severity']==2][column_name].plot(kind='kde',label='Severity = 2')
    data[data['Severity']==3][column_name].plot(kind='kde',label='Severity = 3')
    data[data['Severity']==4][column_name].plot(kind='kde',label='Severity = 4')
    plt.legend()
    plt.show()

continuous_categorical('End_Lat')

continuous_categorical('End_Lng')

continuous_categorical('Temperature(F)')

continuous_categorical('Humidity(%)')

continuous_categorical('Pressure(in)')

continuous_categorical('Wind_Speed(mph)')

continuous_categorical('Precipitation(in)')

continuous_categorical('Distance(mi)')

obj_cols = list(data.select_dtypes(include=['object']).columns)
for col in obj_cols:
    print(f"Column '{col}' has {(len(data[col].unique())*100)/len(data)}% unique values:\n{data[col].unique()}\n")

del data['Description']
del data['Street']
del data['City']
del data['Zipcode']

del data['Airport_Code']

obj_cols = list(data.select_dtypes(include=['object']).columns)
for col in obj_cols:
    print(f"Column '{col}' has {(len(data[col].unique())*100)/len(data)}% unique values:\n{data[col].unique()}\n")

del data['ID']

obj_cols = list(data.select_dtypes(include=['object']).columns)
for col in obj_cols:
    print(f"Column '{col}' has {(len(data[col].unique())*100)/len(data)}% unique values:\n{data[col].unique()}\n")

def get_date_month_year_hour(column_name):
    data[column_name]=pd.to_datetime(data[column_name])
    data[str(column_name)+'Month']=data[column_name].dt.month
    data[str(column_name)+'Year']=data[column_name].dt.year
    data[str(column_name)+'Hour']=data[column_name].dt.hour
    data[str(column_name)+'Weekday']=data[column_name].dt.weekday
    del data[column_name]

get_date_month_year_hour('Start_Time')
get_date_month_year_hour('Weather_Timestamp')
del data['End_Time']

correlation_heatmap()

del data['Start_TimeMonth']
del data['Start_TimeYear']
del data['Start_TimeHour']
del data['Start_TimeWeekday']

null_columns=list(data.columns[data.isnull().any()])
missingno.matrix(data[null_columns],figsize=(10,6));

sample_data=data.sample(int(0.5*len(data)))

severity_2=sample_data[sample_data['Severity']==2]
severity_3=sample_data[sample_data['Severity']==3]
severity_4=sample_data[sample_data['Severity']==4]
fig, ax = plt.subplots()
ax.scatter(severity_2['End_Lng'], severity_2['End_Lat'], c='blue', label='Severity 2',s=1)
ax.scatter(severity_3['End_Lng'], severity_3['End_Lat'], c='green', label='Severity 3',s=1)
ax.scatter(severity_4['End_Lng'], severity_4['End_Lat'], c='red', label='Severity 4',s=1)
ax.legend()
plt.show()

sns.scatterplot(x=sample_data.End_Lng,y=sample_data.End_Lat,size=0.001)

print(data['Timezone'].unique())

sns.countplot(x='Timezone',data=data)

#simple imputer Timezone
print(list(data['Temperature(F)'].value_counts()))
print(data['Temperature(F)'].isnull().sum()/len(data))
#simple imputer Mean Temperature and Humidity, Pressure

print((data['Wind_Direction'].value_counts()))
print(data['Wind_Direction'].isnull().sum()/len(data))
#imputer most frequently

print((data['Precipitation(in)'].value_counts()))
print(data['Precipitation(in)'].isnull().sum()/len(data))
#so fill with 0.00

numeric_columns=['Temperature(F)','Humidity(%)','Pressure(in)','Wind_Speed(mph)']
data[numeric_columns]=data[numeric_columns].fillna(data[numeric_columns].mean())
categorical_columns=['Timezone','Wind_Direction','Weather_Condition','Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight','Weather_TimestampMonth','Weather_TimestampYear','Weather_TimestampHour','Weather_TimestampWeekday']
for col in categorical_columns:
    data[col].fillna(data[col].mode()[0],inplace=True)
data['Precipitation(in)'].fillna(0.00,inplace=True)

print(data.isnull().sum())

print(data.info())

del data['County']

for column in ['Side','State','Timezone','Wind_Direction','Weather_Condition','Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight']:
    print(column)
    print(len(Counter(data[column])))

data_encoded=pd.get_dummies(data,columns=['Side','State','Timezone','Wind_Direction','Weather_Condition','Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight'],drop_first=True)

data=data_encoded.copy()

output_data=data['Severity']
del data['Severity']
input_data=data

print(input_data.info())

print(input_data.shape)

x_train,x_test,y_train,y_test=train_test_split(input_data,output_data,train_size=0.75,random_state=0)
x_train,x_validation,y_train,y_validation=train_test_split(x_train,y_train,train_size=60/75,random_state=1)

columns=['End_Lat','End_Lng','Distance(mi)','Temperature(F)','Humidity(%)','Pressure(in)','Wind_Speed(mph)','Precipitation(in)','Month','Year','Hour','Weekday','Weather_TimestampMonth','Weather_TimestampYear','Weather_TimestampHour','Weather_TimestampWeekday']
scaler=StandardScaler()
scaler.fit(x_train[columns])
x_train[columns]=(scaler.transform(x_train[columns]))
x_validation[columns]=(scaler.transform(x_validation[columns]))
x_test[columns]=(scaler.transform(x_test[columns]))



"""## decision tree"""

# Create a Decision Tree Classifier
dtc = DecisionTreeClassifier(random_state=0)

# Fit the classifier to the training data
dtc.fit(x_train, y_train)

print("Training accuracy:", dtc.score(x_train, y_train))
print("Testing accuracy:", dtc.score(x_test, y_test)) 

# Evaluate the classifier on the testing data
y_pred = dtc.predict(x_test)
print("Accuracy:", accuracy_score(y_test, y_pred))


cm = confusion_matrix(y_test, y_pred)

# Create a heatmap of the confusion matrix
sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()



# Plot the decision tree
plt.figure(figsize=(36,24))
plot_tree(dtc, filled=True)
plt.show()

# Get the feature importances using MDI
importances = dtc.feature_importances_
sorted_idx = importances.argsort()
col=list(input_data.columns)
plt.figure(figsize=(10,100))
# Plot the feature importances using MDI with Seaborn
sorted_idx=np.flip(sorted_idx)
sns.barplot(x=[importances[i] for i in sorted_idx], y=[col[i] for i in sorted_idx])
sns.set(style='whitegrid')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.show()

pip install yellowbrick

visualizer = ClassificationReport(dtc, classes=list(Counter(y_train).keys()).sort(), support=True)

visualizer.fit(x_train, y_train)        # Fit the visualizer and the model
visualizer.score(x_test, y_test)        # Evaluate the model on the test data
visualizer.show()

from sklearn.model_selection import GridSearchCV

# Define the hyperparameters to search over
param_grid = {
    'max_depth': [2, 4, 6],
    'min_samples_split': [2, 4, 6],
    'min_samples_leaf': [1, 2, 3],
    'max_features': ['auto', 'sqrt', 'log2'],
    'criterion': ['gini', 'entropy']
}

# Define the grid search object
grid_search = GridSearchCV(dtc, param_grid, cv=5, scoring='accuracy')

# Train the grid search object
grid_search.fit(x_train, y_train)

# Print the best hyperparameters
print(grid_search.best_params_)

print(grid_search.best_score_)

# Make predictions on the test dataset using the best model
best_model = grid_search.best_estimator_
y_pred = best_model.predict(x_test)

print(classification_report(y_test, y_pred))



"""## GaussianNB"""

gnb = GaussianNB()
gnb.fit(x_train, y_train)

print("Training accuracy:", gnb.score(x_train, y_train))
print("Testing accuracy:", gnb.score(x_test, y_test)) 
# Make predictions on the testing set
y_pred = gnb.predict(x_test)
bal_acc = balanced_accuracy_score(y_test, y_pred)

print("Balanced accuracy:", bal_acc)

# Get ROC scores for one-vs-one classification
roc_ovo = roc_auc_score(y_test, gnb.predict_proba(x_test), multi_class='ovo')
print("ROC score (one-vs-one):", roc_ovo)

# Get ROC scores for one-vs-rest classification
roc_ovr = roc_auc_score(y_test, gnb.predict_proba(x_test), multi_class='ovr')
print("ROC score (one-vs-rest):", roc_ovr)

# Compute and print the confusion matrix
cm = confusion_matrix(y_test, y_pred)


# Create a heatmap of the confusion matrix
sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# Compute and print the classification report
cr = classification_report(y_test, y_pred)
print("Classification Report:\n", cr)

visualizer = ClassificationReport(gnb, support=True, cmap='PuBu')
visualizer.fit(x_train, y_train)
visualizer.score(x_test, y_test)

visualizer.show()

from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10]
}

# Instantiate the grid search model
grid_search = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid, cv=5)

# Fit the grid search to the training data
grid_search.fit(x_train, y_train)

# Print the best parameter and the corresponding score
print("Best parameter:", grid_search.best_params_)
print("Best score:", grid_search.best_score_)

# Make predictions on the test dataset using the best model
best_model = grid_search.best_estimator_
y_pred = best_model.predict(x_test)

print(classification_report(y_test, y_pred))



"""## Random Forest



"""

# Create the Random Forest model
rf = RandomForestClassifier(n_estimators=100, random_state=0)

# Fit the model to the data
rf.fit(x_train, y_train)

y_pred = rf.predict(x_test)
# Get balanced accuracy score
bal_acc = balanced_accuracy_score(y_test, y_pred)

print("Training accuracy:", rf.score(x_train, y_train))
print("Testing accuracy:", rf.score(x_test, y_test)) 

print("Balanced accuracy:", bal_acc)

# Get ROC scores for one-vs-one classification
roc_ovo = roc_auc_score(y_test, rf.predict_proba(x_test), multi_class='ovo')
print("ROC score (one-vs-one):", roc_ovo)

# Get ROC scores for one-vs-rest classification
roc_ovr = roc_auc_score(y_test, rf.predict_proba(x_test), multi_class='ovr')
print("ROC score (one-vs-rest):", roc_ovr)

cm = confusion_matrix(y_test, y_pred)

cr = classification_report(y_test, y_pred, zero_division=1)
print("Classification Report:")
print(cr)

# Create a heatmap of the confusion matrix
sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

visualizer = ClassificationReport(rf, support=True)

visualizer.fit(x_train, y_train)        # Fit the visualizer and the model
visualizer.score(x_test, y_test)        # Evaluate the model on the test data
visualizer.show()

# Get the feature importances using MDI
importances = rf.feature_importances_
sorted_idx = importances.argsort()
col=list(input_data.columns)
plt.figure(figsize=(10,100))
# Plot the feature importances using MDI with Seaborn
sorted_idx=np.flip(sorted_idx)
sns.barplot(x=[importances[i] for i in sorted_idx], y=[col[i] for i in sorted_idx])
sns.set(style='whitegrid')
plt.xlabel('Importance')
plt.show()



"""## Logistic Regression"""

lr = LogisticRegression(max_iter=1000)
lr.fit(x_train, y_train)

print("Training accuracy:", lr.score(x_train, y_train))
print("Testing accuracy:", lr.score(x_test, y_test))


y_pred = lr.predict(x_test)

# Get balanced accuracy score
bal_acc = balanced_accuracy_score(y_test, y_pred)
print("Balanced accuracy:", bal_acc)

# Get ROC scores for one-vs-one classification
roc_ovo = roc_auc_score(y_test, lr.predict_proba(x_test), multi_class='ovo')
print("ROC score (one-vs-one):", roc_ovo)

# Get ROC scores for one-vs-rest classification
roc_ovr = roc_auc_score(y_test, lr.predict_proba(x_test), multi_class='ovr')
print("ROC score (one-vs-rest):", roc_ovr)

cm = confusion_matrix(y_test, y_pred)

# Create a heatmap of the confusion matrix
sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

cr = classification_report(y_test, y_pred, zero_division=1)
print("Classification Report:")
print(cr)

visualizer = ClassificationReport(lr, support=True, cmap='PuBu')
visualizer.fit(x_train, y_train)
visualizer.score(x_test, y_test)

visualizer.show()

# Get the feature importances using MDI
coefficients = lr.coef_[0]
feature_importance = pd.Series(coefficients, index=col)
plt.figure(figsize=(10,100))
feature_importance.plot(kind='barh', color='blue')
plt.title('Feature Importance')
plt.show()



"""## Gradient Boosting"""

# Fit a Gradient Boosting Tree classifier
gbt = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=0)
gbt.fit(x_train, y_train)
y_pred = gbt.predict(x_test)


print("Training accuracy:", gbt.score(x_train, y_train))
print("Testing accuracy:", gbt.score(x_test, y_test))

bal_acc = balanced_accuracy_score(y_test, y_pred)
print("Balanced accuracy:", bal_acc)

# Compute the one-vs-one macro accuracy
ovo_acc = roc_auc_score(y_test, gbt.predict_proba(x_test), multi_class="ovo", average="macro")
print("One-vs-one macro accuracy:", ovo_acc)

# Compute the one-vs-rest macro accuracy
ovr_acc = roc_auc_score(y_test, gbt.predict_proba(x_test), multi_class="ovr", average="macro")
print("One-vs-rest macro accuracy:", ovr_acc)

cm = confusion_matrix(y_test, y_pred)

# Create a heatmap of the confusion matrix
sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# Compute the classification report, which includes precision, recall, f1-score, and support
cr = classification_report(y_test, y_pred, zero_division=1)
print("Classification Report:")
print(cr)

visualizer = ClassificationReport(gbt, support=True, cmap='PuBu')
visualizer.fit(x_train, y_train)
visualizer.score(x_test, y_test)

visualizer.show()

# Get the feature importances using MDI
importances = gbt.feature_importances_
sorted_idx = importances.argsort()
col=list(input_data.columns)
plt.figure(figsize=(10,100))
# Plot the feature importances using MDI with Seaborn
sorted_idx=np.flip(sorted_idx)
sns.barplot(x=[importances[i] for i in sorted_idx], y=[col[i] for i in sorted_idx])
sns.set(style='whitegrid')
plt.xlabel('Importance')
plt.show()



"""## Xg Boosting"""

# Create an XGBoost classifier
xgb = XGBClassifier()

# Fit the classifier to the training data
y_new_train=y_train-2
y_new_test=y_test-2
xgb.fit(x_train, y_new_train)
print("Training accuracy:", xgb.score(x_train, y_new_train))
print("Testing accuracy:", xgb.score(x_test, y_new_test))
# Evaluate the classifier on the testing data
y_pred = xgb.predict(x_test)
print("Accuracy:", accuracy_score(y_new_test, y_pred))

y_pred = xgb.predict(x_test)

# Calculate balanced accuracy
balanced_acc = balanced_accuracy_score(y_new_test, y_pred)

# Calculate ROC one-vs-one macro accuracy
roc_ovo_macro = roc_auc_score(y_new_test, xgb.predict_proba(x_test), multi_class='ovo', average='macro')

# Calculate ROC one-vs-rest accuracy
roc_ovr_macro = roc_auc_score(y_new_test, xgb.predict_proba(x_test), multi_class='ovr', average='macro')

# Print the results
print("Balanced accuracy:", balanced_acc)
print("ROC one-vs-one macro accuracy:", roc_ovo_macro)
print("ROC one-vs-rest macro accuracy:", roc_ovr_macro)



cm = confusion_matrix(y_new_test, y_pred)

# Create a heatmap of the confusion matrix
sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# Compute the classification report, which includes precision, recall, f1-score, and support
cr = classification_report(y_new_test, y_pred)
print("Classification Report:")
print(cr)

visualizer = ClassificationReport(xgb, support=True, cmap='PuBu')
visualizer.fit(x_train, y_new_train)
visualizer.score(x_test, y_new_test)

visualizer.show()

# Get the feature importances using MDI
importances = xgb.feature_importances_
sorted_idx = importances.argsort()
col=list(input_data.columns)
plt.figure(figsize=(10,100))
# Plot the feature importances using MDI with Seaborn
sorted_idx=np.flip(sorted_idx)
sns.barplot(x=[importances[i] for i in sorted_idx], y=[col[i] for i in sorted_idx])
sns.set(style='whitegrid')
plt.xlabel('Importance')
plt.show()



"""## KNN"""

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(x_train, y_train)

print("Training accuracy:", knn.score(x_train, y_train))
print("Testing accuracy:", knn.score(x_test, y_test))

y_pred = knn.predict(x_test)

bal_acc = balanced_accuracy_score(y_test, y_pred)
print("Balanced accuracy:", bal_acc)

# Compute the one-vs-one macro accuracy
ovo_acc = roc_auc_score(y_test, knn.predict_proba(x_test), multi_class="ovo", average="macro")
print("One-vs-one macro accuracy:", ovo_acc)

# Compute the one-vs-rest macro accuracy
ovr_acc = roc_auc_score(y_test, knn.predict_proba(x_test), multi_class="ovr", average="macro")
print("One-vs-rest macro accuracy:", ovr_acc)
cm = confusion_matrix(y_test, y_pred)

# Create a heatmap of the confusion matrix
sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# Compute the classification report, which includes precision, recall, f1-score, and support
cr = classification_report(y_test, y_pred, zero_division=1)
print("Classification Report:")
print(cr)

visualizer = ClassificationReport(knn, support=True, cmap='PuBu')
visualizer.fit(x_train, y_train)
visualizer.score(x_test, y_test)

visualizer.show()



"""# AdaBoost"""

adaboost = AdaBoostClassifier(n_estimators=100, learning_rate=0.1, random_state=0)
adaboost.fit(x_train, y_train)

print("Training accuracy:", adaboost.score(x_train, y_train))
print("Testing accuracy:", adaboost.score(x_test, y_test))

y_pred = adaboost.predict(x_test)

bal_acc = balanced_accuracy_score(y_test, y_pred)
print("Balanced accuracy:", bal_acc)

# Get ROC scores for one-vs-one classification
roc_ovo = roc_auc_score(y_test, adaboost.predict_proba(x_test), multi_class='ovo')
print("ROC score (one-vs-one):", roc_ovo)

# Get ROC scores for one-vs-rest classification
roc_ovr = roc_auc_score(y_test, adaboost.predict_proba(x_test), multi_class='ovr')
print("ROC score (one-vs-rest):", roc_ovr)

cm = confusion_matrix(y_test, y_pred)

# Create a heatmap of the confusion matrix
sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# Compute the classification report, which includes precision, recall, f1-score, and support
cr = classification_report(y_test, y_pred, zero_division=1)
print("Classification Report:")
print(cr)

visualizer = ClassificationReport(adaboost, support=True, cmap='PuBu')
visualizer.fit(x_train, y_train)
visualizer.score(x_test, y_test)

visualizer.show()

# Get the feature importances using MDI
importances = adaboost.feature_importances_
sorted_idx = importances.argsort()
col=list(input_data.columns)
plt.figure(figsize=(10,100))
# Plot the feature importances using MDI with Seaborn
sorted_idx=np.flip(sorted_idx)
sns.barplot(x=[importances[i] for i in sorted_idx], y=[col[i] for i in sorted_idx])
sns.set(style='whitegrid')
plt.xlabel('Importance')
plt.show()



"""# lightGBM"""

lgb_clf = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, random_state=0)
lgb_clf.fit(x_train, y_train)

print("Training accuracy:", lgb_clf.score(x_train, y_train))
print("Testing accuracy:", lgb_clf.score(x_test, y_test))

y_pred = lgb_clf.predict(x_test)
bal_acc = balanced_accuracy_score(y_test, y_pred)
print("Balanced accuracy:", bal_acc)

# Get ROC scores for one-vs-one classification
roc_ovo = roc_auc_score(y_test, lgb_clf.predict_proba(x_test), multi_class='ovo')
print("ROC score (one-vs-one):", roc_ovo)

# Get ROC scores for one-vs-rest classification
roc_ovr = roc_auc_score(y_test, lgb_clf.predict_proba(x_test), multi_class='ovr')
print("ROC score (one-vs-rest):", roc_ovr)
cm = confusion_matrix(y_test, y_pred)

# Create a heatmap of the confusion matrix
sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# Compute the classification report, which includes precision, recall, f1-score, and support
cr = classification_report(y_test, y_pred, zero_division=1)
print("Classification Report:")
print(cr)

visualizer = ClassificationReport(lgb_clf, support=True, cmap='PuBu')
visualizer.fit(x_train, y_train)
visualizer.score(x_test, y_test)

visualizer.show()

# Get the feature importances using MDI
importances = lgb_clf.feature_importances_
sorted_idx = importances.argsort()
col=list(input_data.columns)
plt.figure(figsize=(10,100))
# Plot the feature importances using MDI bwith Seaborn
sorted_idx=np.flip(sorted_idx)
sns.barplot(x=[importances[i] for i in sorted_idx], y=[col[i] for i in sorted_idx])
sns.set(style='whitegrid')
plt.xlabel('Importance')
plt.show()



"""# CatBoost"""

catboost = CatBoostClassifier(n_estimators=100, learning_rate=0.1, random_state=0)
catboost.fit(x_train, y_train,verbose=False)

print("Training accuracy:", catboost.score(x_train, y_train))
print("Testing accuracy:", catboost.score(x_test, y_test))
y_pred = catboost.predict(x_test)

bal_acc = balanced_accuracy_score(y_test, y_pred)
print("Balanced accuracy:", bal_acc)

# Get ROC scores for one-vs-one classification
roc_ovo = roc_auc_score(y_test, catboost.predict_proba(x_test), multi_class='ovo')
print("ROC score (one-vs-one):", roc_ovo)

# Get ROC scores for one-vs-rest classification
roc_ovr = roc_auc_score(y_test, catboost.predict_proba(x_test), multi_class='ovr')
print("ROC score (one-vs-rest):", roc_ovr)
cm = confusion_matrix(y_test, y_pred)

# Create a heatmap of the confusion matrix
sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# Compute the classification report, which includes precision, recall, f1-score, and support
cr = classification_report(y_test, y_pred, zero_division=1)
print("Classification Report:")
print(cr)

# Get the feature importances using MDI
importances = catboost.feature_importances_
sorted_idx = importances.argsort()
col=list(input_data.columns)
plt.figure(figsize=(10,100))
# Plot the feature importances using MDI bwith Seaborn
sorted_idx=np.flip(sorted_idx)
sns.barplot(x=[importances[i] for i in sorted_idx], y=[col[i] for i in sorted_idx])
sns.set(style='whitegrid')
plt.xlabel('Importance')
plt.show()

y_train=np.array(pd.get_dummies(y_train))
y_validation=np.array(pd.get_dummies(y_validation))
y_test=np.array(pd.get_dummies(y_test))

x_train=tensorflow.convert_to_tensor(x_train,dtype=tensorflow.float32)
x_validation=tensorflow.convert_to_tensor(x_validation,dtype=tensorflow.float32)
x_test=tensorflow.convert_to_tensor(x_test,dtype=tensorflow.float32)

import os
os.environ['TF_KERAS_TUNER_FLAG'] = '1'
def build_model(hp):
    model=Sequential()
    model.add(Dense(units=hp.Int('input_units',min_value=32,max_value=512,step=32),
                           activation=hp.Choice('input_activation', values=['relu', 'tanh', 'sigmoid']),
                           input_shape=(len(data.columns),)))
    for i in range(hp.Int('num_layers',min_value=1,max_value=10)):
        model.add(Dense(units=hp.Int(f'layer_{i}_units', min_value=32, max_value=512, step=32),
                               activation=hp.Choice(f'layer_{i}_activation', values=['relu', 'tanh', 'sigmoid']),
                               kernel_regularizer=hp.Choice(f'layer_{i}_kernel_regularizer', values=['l1', 'l2']),
                               bias_regularizer=hp.Choice(f'layer_{i}_bias_regularizer', values=['l1', 'l2'])))
        model.add(Dropout(hp.Float(f'layer_{i}_dropout', min_value=0.0, max_value=0.9, step=0.1)))
        if hp.Boolean(f'layer_{i}_batch_normalization'):
            model.add(BatchNormalization())
    model.add(Dense(units=3, activation='softmax'))
    model.compile(optimizer=hp.Choice('optimizer',values=['adam','rmsprop', 'sgd']),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model

tuner = kt.RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    directory='My_Final_Directory_2023',
    project_name='my_project'
)
early_stopping=keras.callbacks.EarlyStopping(
    monitor='val_accuracy',
    patience=3,
    restore_best_weights=True
)

tuner.search(x=x_train,y=y_train,
             epochs=5,
             validation_data=(x_validation,y_validation),
             callbacks=[early_stopping]
        )

best_hp=tuner.get_best_hyperparameters()[0]
severity_model=build_model(best_hp)

severity_model.fit(x_train,y_train,epochs=100,initial_epoch=6,validation_data=(x_validation,y_validation),callbacks=early_stopping)

import pickle 
with open('severity_model','wb') as f:
  pickle.dump(severity_model,f)

import pickle
with open('severity_model','rb') as f:
    severity_model=pickle.load(f)

y_predicted=np.argmax(severity_model.predict(x_test),axis=1)
y_predicted+=2

y_test=np.argmax(y_test,axis=1)
y_test+=2

from sklearn.metrics import accuracy_score
print(f'Testing Accuracy for our Model is {accuracy_score(y_predicted,y_test)}')

#location(regression) prediction starts here

data=pd.read_csv('/content/drive/MyDrive/major_project_data.csv',nrows=5e5)

print(data.columns)

data.End_Time=pd.to_datetime(data.End_Time)
data['Month']=data['End_Time'].dt.month
data['Year']=data['End_Time'].dt.year
data['Hour']=data['End_Time'].dt.hour
data['Weekday']=data['End_Time'].dt.weekday

del data['Country']
del data['Turning_Loop']
del data['Roundabout']
del data['Bump']

correlation_heatmap()

print(data.columns)

correlation_heatmap()

def delete_low_corr_cols(data):
    corr_matrix=data.corr()
    severity_corr=corr_matrix['End_Lat']
    low_corr_cols=severity_corr[abs(severity_corr)<0.01].index.tolist()
    data=data.drop(columns=low_corr_cols)
    print(low_corr_cols)
    corr_matrix=data.corr()
    severity_corr=corr_matrix['End_Lng']
    low_corr_cols=severity_corr[abs(severity_corr)<0.01].index.tolist()
    print(low_corr_cols)
    data=data.drop(columns=low_corr_cols)
    return data

data=delete_low_corr_cols(data)

correlation_heatmap()

del data['ID']
del data['Airport_Code']
del data['Description']
del data['Street']
del data['City']
del data['Zipcode']
del data['County']

get_date_month_year_hour('Start_Time')
get_date_month_year_hour('Weather_Timestamp')
del data['End_Time']

correlation_heatmap()

del data['Start_TimeMonth']
del data['Start_TimeYear']
del data['Start_TimeHour']
del data['Start_TimeWeekday']

numeric_columns=['Temperature(F)','Pressure(in)','Wind_Speed(mph)']
data[numeric_columns]=data[numeric_columns].fillna(data[numeric_columns].mean())
categorical_columns=['Timezone','Wind_Direction','Weather_Condition','Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight','Weather_TimestampMonth','Weather_TimestampYear','Weather_TimestampHour','Weather_TimestampWeekday']
for col in categorical_columns:
    data[col].fillna(data[col].mode()[0],inplace=True)

data.isnull().sum()

del data['Number']
data['Visibility(mi)'].fillna(data['Visibility(mi)'].mean(),inplace=True)

data.isnull().sum()

print(data.info())

data=pd.get_dummies(data,columns=['Side','State','Timezone','Wind_Direction','Weather_Condition','Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight'],drop_first=True)

input_data=data.drop(['End_Lat','End_Lng'],axis=1)
output_data=data[['End_Lat','End_Lng']]

x_train,x_test,y_train,y_test=train_test_split(input_data,output_data,train_size=0.75,random_state=0)
x_train,x_validation,y_train,y_validation=train_test_split(x_train,y_train,train_size=60/75,random_state=1)



"""## numerical_num"""

continuous_col=['Start_Lat','Start_Lng','Distance(mi)','Number','Temperature(F)','Wind_Chill(F)','Humidity(%)','Pressure(in)','Visibility(mi)','Precipitation(in)','Wind_Speed(mph)']

def scatter_plot(col1,col2):
  x = data[col1]
  y = data[col2]
  plt.scatter(x, y,s=2,c=np.random.rand(3,))

  # set plot title and axis labels
  t="Scatter Plot of "+str(col1)+" and "+str(col2)
  plt.title(t)
  plt.xlabel(col1)
  plt.ylabel(col2)

  # show the plot
  plt.show()

for i in continuous_col:
  scatter_plot('End_Lat',i)

for i in continuous_col:
  scatter_plot('End_Lng',i)

import seaborn as sns

def regression_plot(col1,col2):
  
  sns.set_style('whitegrid')
  sns.lmplot(x =col1, y =col2, data = data, 
            hue ='Severity', markers =['o', 'v','+'])

for i in continuous_col:
  regression_plot('End_Lat',i)

for i in continuous_col:
  regression_plot('End_Lng',i)

def hist_2d(col1, col2):
    col1_clean = data[col1].fillna(data[col1].mean())
    col2_clean = data[col2].fillna(data[col2].mean())
    plt.hist2d(col1_clean, col2_clean, bins=30, cmap='coolwarm')
    plt.colorbar()
    plt.xlabel(col1)
    plt.ylabel(col2)
    plt.show()

for i in continuous_col:
  hist_2d('End_Lat',i)

for i in continuous_col:
  hist_2d('End_Lng',i)

def box_plot(col1,col2):
  df = pd.DataFrame().assign(x=data[col1], y=data[col2])
  df.rename(columns = {'x':col1,'y':col2}, inplace = True)
  df.boxplot()
  plt.ylabel('Value')
  plt.show()

for i in continuous_col:
  box_plot('End_Lat',i)

for i in continuous_col:
  box_plot('End_Lng',i)



"""## cat_num"""

categorical_col=['Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station','Stop','Traffic_Calming','Traffic_Signal','Turning_Loop','Country','State','Side','Timezone','Wind_Direction','Weather_Condition','Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight']

import seaborn as sns

fig, axs = plt.subplots(nrows=len(categorical_col), figsize=(8, 6*len(categorical_col)))
for i, col in enumerate(categorical_col):
    ax = axs[i]
    sns.violinplot(x=col, y='End_Lat', data=data, ax=ax)
    ax.set_xlabel(col)
    ax.set_ylabel('End_Lat')

plt.tight_layout()
plt.show()

import seaborn as sns

fig, axs = plt.subplots(nrows=len(categorical_col), figsize=(8, 6*len(categorical_col)))
for i, col in enumerate(categorical_col):
    ax = axs[i]
    sns.violinplot(x=col, y='End_Lng', data=data, ax=ax)
    ax.set_xlabel(col)
    ax.set_ylabel('End_Lng')

plt.tight_layout()
plt.show()

import seaborn as sns

fig, axs = plt.subplots(nrows=len(categorical_col), figsize=(8, 6*len(categorical_col)))
for i, col in enumerate(categorical_col):
    ax = axs[i]
    sns.barplot(x=col, y='End_Lat', data=data, ax=ax)
    ax.set_xlabel(col)
    ax.set_ylabel('End_Lat')

plt.tight_layout()
plt.show()

import seaborn as sns

fig, axs = plt.subplots(nrows=len(categorical_col), figsize=(8, 6*len(categorical_col)))
for i, col in enumerate(categorical_col):
    ax = axs[i]
    sns.barplot(x=col, y='End_Lng', data=data, ax=ax)
    ax.set_xlabel(col)
    ax.set_ylabel('End_Lat')

plt.tight_layout()
plt.show()

import seaborn as sns

fig, axs = plt.subplots(nrows=len(categorical_col), figsize=(8, 6*len(categorical_col)))
for i, col in enumerate(categorical_col):
    ax = axs[i]
    sns.boxplot(x=col, y='End_Lat', data=data, ax=ax,hue="Severity",palette={2: "magenta", 3: "blue",4:"pink"})
    ax.set_xlabel(col)
    ax.set_ylabel('End_Lat')

plt.tight_layout()
plt.show()

import seaborn as sns

fig, axs = plt.subplots(nrows=len(categorical_col), figsize=(8, 6*len(categorical_col)))
for i, col in enumerate(categorical_col):
    ax = axs[i]
    sns.boxplot(x=col, y='End_Lng', data=data, ax=ax,hue="Severity",palette={2: "red", 3: "yellow",4:"green"})
    ax.set_xlabel(col)
    ax.set_ylabel('End_Lng')

plt.tight_layout()
plt.show()

data.End_Time=pd.to_datetime(data.End_Time)
data['Month']=data['End_Time'].dt.month
data['Year']=data['End_Time'].dt.year
data['Hour']=data['End_Time'].dt.hour
data['Weekday']=data['End_Time'].dt.weekday

del data['Country']
del data['Turning_Loop']
del data['Roundabout']
del data['Bump']

correlation_heatmap()

print(data.columns)

correlation_heatmap()

def delete_low_corr_cols(data):
    corr_matrix=data.corr()
    severity_corr=corr_matrix['End_Lat']
    low_corr_cols=severity_corr[abs(severity_corr)<0.01].index.tolist()
    data=data.drop(columns=low_corr_cols)
    print(low_corr_cols)
    corr_matrix=data.corr()
    severity_corr=corr_matrix['End_Lng']
    low_corr_cols=severity_corr[abs(severity_corr)<0.01].index.tolist()
    print(low_corr_cols)
    data=data.drop(columns=low_corr_cols)
    return data

data=delete_low_corr_cols(data)

del data['ID']
del data['Airport_Code']
del data['Description']
del data['Street']
del data['City']
del data['Zipcode']
del data['County']

get_date_month_year_hour('Start_Time')
get_date_month_year_hour('Weather_Timestamp')
del data['End_Time']

correlation_heatmap()

del data['Start_TimeMonth']
del data['Start_TimeYear']
del data['Start_TimeHour']
del data['Start_TimeWeekday']

numeric_columns=['Temperature(F)','Pressure(in)','Wind_Speed(mph)']
data[numeric_columns]=data[numeric_columns].fillna(data[numeric_columns].mean())
categorical_columns=['Timezone','Wind_Direction','Weather_Condition','Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight','Weather_TimestampMonth','Weather_TimestampYear','Weather_TimestampHour','Weather_TimestampWeekday']
for col in categorical_columns:
    data[col].fillna(data[col].mode()[0],inplace=True)

data.isnull().sum()

del data['Number']
data['Visibility(mi)'].fillna(data['Visibility(mi)'].mean(),inplace=True)

data.isnull().sum()

print(data.info())

data=pd.get_dummies(data,columns=['Side','State','Timezone','Wind_Direction','Weather_Condition','Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight'],drop_first=True)

input_data=data.drop(['End_Lat','End_Lng'],axis=1)
output_data=data[['End_Lat','End_Lng']]

x_train,x_test,y_train,y_test=train_test_split(input_data,output_data,train_size=0.75,random_state=0)
x_train,x_validation,y_train,y_validation=train_test_split(x_train,y_train,train_size=60/75,random_state=1)

columns=['Distance(mi)','Temperature(F)','Visibility(mi)','Pressure(in)','Wind_Speed(mph)','Year','Weather_TimestampMonth','Weather_TimestampYear','Weather_TimestampHour','Weather_TimestampWeekday']
scaler=StandardScaler()
scaler.fit(x_train[columns])
x_train[columns]=(scaler.transform(x_train[columns]))
x_validation[columns]=(scaler.transform(x_validation[columns]))
x_test[columns]=(scaler.transform(x_test[columns]))

x_train=tensorflow.convert_to_tensor(x_train,dtype=tensorflow.float32)
x_validation=tensorflow.convert_to_tensor(x_validation,dtype=tensorflow.float32)
x_test=tensorflow.convert_to_tensor(x_test,dtype=tensorflow.float32)

print(x_train.shape)

def build_model(hp):
    model=Sequential()
    model.add(Dense(units=hp.Int('input_units',min_value=32,max_value=512,step=32),
                           activation=hp.Choice('input_activation', values=['relu', 'tanh', 'sigmoid']),
                           input_shape=(len(input_data.columns),)))
    for i in range(hp.Int('num_layers',min_value=1,max_value=10)):
        model.add(Dense(units=hp.Int(f'layer_{i}_units', min_value=32, max_value=512, step=32),
                               activation=hp.Choice(f'layer_{i}_activation', values=['relu', 'tanh', 'sigmoid']),
                               kernel_regularizer=hp.Choice(f'layer_{i}_kernel_regularizer', values=['l1', 'l2']),
                               bias_regularizer=hp.Choice(f'layer_{i}_bias_regularizer', values=['l1', 'l2'])))
        model.add(Dropout(hp.Float(f'layer_{i}_dropout', min_value=0.0, max_value=0.9, step=0.1)))
        if hp.Boolean(f'layer_{i}_batch_normalization'):
            model.add(BatchNormalization())
    model.add(Dense(units=2,activation='linear'))
    model.compile(optimizer=hp.Choice('optimizer',values=['adam','rmsprop', 'sgd']),
                  loss='mean_squared_error',
                  metrics=['mse'])

    return model

tuner = kt.RandomSearch(
    build_model,
    objective='val_loss',
    max_trials=5,
    directory='My_final_directory_3000',
    project_name='my_project'
)
early_stopping=keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

tuner.search(x=x_train,y=y_train,
             epochs=5,
             validation_data=(x_validation,y_validation),
             callbacks=[early_stopping]
        )

best_hp=tuner.get_best_hyperparameters()[0]
location_model=build_model(best_hp)

location_model.fit(x_train,y_train,epochs=100,initial_epoch=6,validation_data=(x_validation,y_validation),callbacks=early_stopping)

import pickle 
with open('location_model','wb') as f:
  pickle.dump(location_model,f)

y_predicted=location_model.predict(x_test)

print(y_predicted)

print(y_test)

mse=tensorflow.keras.losses.mean_squared_error(y_test,y_predicted)

print(f'Testing Mean Squared Error is {np.array(mse).mean()}')

data=pd.read_csv('/content/drive/MyDrive/major_project_data.csv',nrows=5e5)

class DropColumnsTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, columns_to_drop):
        self.columns_to_drop = columns_to_drop
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        return X.drop(self.columns_to_drop, axis=1)

class DateTimeFeatureTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, column_name):
        self.column_name = column_name

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        X[self.column_name] = pd.to_datetime(X[self.column_name])
        X[str(self.column_name)+'Month'] = X[self.column_name].dt.month
        X[str(self.column_name)+'Year'] = X[self.column_name].dt.year
        X[str(self.column_name)+'Hour'] = X[self.column_name].dt.hour
        X[str(self.column_name)+'Weekday'] = X[self.column_name].dt.weekday
        del X[self.column_name]
        return X

columns_to_drop=['Bump','Roundabout','Turning_Loop','Country','Number', 'Visibility(mi)', 'Amenity', 'Bump', 'No_Exit', 'Railway', 'Roundabout', 'Stop', 'Traffic_Calming','Wind_Chill(F)','Start_Lat','Start_Lng','Description','Street',
'City','Zipcode','Airport_Code','ID','County','Start_TimeMonth','Start_TimeYear','Start_TimeHour','Start_TimeWeekday']

numeric_columns=['End_Lat','End_Lng','Distance(mi)','Temperature(F)','Humidity(%)','Pressure(in)','Wind_Speed(mph)','Precipitation(in)','End_TimeMonth','End_TimeYear','End_TimeHour','End_TimeWeekday','Weather_TimestampMonth','Weather_TimestampYear','Weather_TimestampHour','Weather_TimestampWeekday']
categorical_columns=['Side','State','Timezone','Wind_Direction','Weather_Condition','Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight']

numeric_transformer=Pipeline(steps=[
    ('standardize',StandardScaler()),
    ('imputer',SimpleImputer(strategy='mean')),
    ('precipitation_imputer',SimpleImputer(strategy='constant',fill_value=0.00))
])
categorical_transformer=Pipeline(steps=[
    ('imputer',SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(drop='first',handle_unknown='ignore'))
])
preprocessor=ColumnTransformer(
    transformers=[
        ('num',numeric_transformer,numeric_columns),
        ('cat',categorical_transformer,categorical_columns)
    ],remainder='passthrough')

output_data=data['Severity']
del data['Severity']
input_data=data

severity_pipeline=Pipeline([
    ('datetime_feature_transformer_weather',DateTimeFeatureTransformer('Weather_Timestamp')),
    ('datetime_feature_transformer_end_time',DateTimeFeatureTransformer('End_Time')),
    ('datetime_feature_transformer_start_time',DateTimeFeatureTransformer('Start_Time')),
    ('drop_columns',DropColumnsTransformer(columns_to_drop)),
    ('preprocessor',preprocessor),
])

input_data=severity_pipeline.fit_transform(input_data)

x_train,x_test,y_train,y_test=train_test_split(input_data,output_data,train_size=0.75,random_state=0)
x_train,x_validation,y_train,y_validation=train_test_split(x_train,y_train,train_size=60/75,random_state=1)

x_train=x_train.toarray()
x_validation=x_validation.toarray()
x_test=x_test.toarray()

y_train=np.array(pd.get_dummies(y_train))
y_validation=np.array(pd.get_dummies(y_validation))
y_test=np.array(pd.get_dummies(y_test))

x_train=tensorflow.convert_to_tensor(x_train,dtype=tensorflow.float32)
x_validation=tensorflow.convert_to_tensor(x_validation,dtype=tensorflow.float32)
x_test=tensorflow.convert_to_tensor(x_test,dtype=tensorflow.float32)

with open('severity_model','rb') as f:
    severity_model=pickle.load(f)

y_predicted=np.argmax(severity_model.predict(x_test),axis=1)
y_test=np.argmax(y_test,axis=1)

#Visualize pipeline
set_config(display='diagram')
severity_pipeline

with open('severity_pipeline','wb') as f:
  pickle.dump(severity_pipeline,f)

data=pd.read_csv('/content/drive/MyDrive/major_project_data.csv',nrows=5e5)

#Location pipeline
class DropColumnsTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, columns_to_drop):
        self.columns_to_drop = columns_to_drop
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        return X.drop(self.columns_to_drop, axis=1)

class DateTimeFeatureTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, column_name):
        self.column_name = column_name

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        X[self.column_name] = pd.to_datetime(X[self.column_name])
        X[str(self.column_name)+'Month'] = X[self.column_name].dt.month
        X[str(self.column_name)+'Year'] = X[self.column_name].dt.year
        X[str(self.column_name)+'Hour'] = X[self.column_name].dt.hour
        X[str(self.column_name)+'Weekday'] = X[self.column_name].dt.weekday
        del X[self.column_name]
        return X

columns_to_drop=['Country','Turning_Loop','Roundabout','Bump','Humidity(%)', 'Precipitation(in)', 'Amenity', 'Give_Way', 'Railway', 'Traffic_Calming','Wind_Chill(F)', 'No_Exit', 'End_TimeMonth', 'End_TimeHour', 'End_TimeWeekday','ID','Airport_Code','Description','Street','City','Zipcode','County','Start_TimeYear','Start_TimeHour','Start_TimeWeekday','Start_TimeMonth','Number']

numeric_columns=['Distance(mi)','Temperature(F)','Visibility(mi)','End_TimeYear','Pressure(in)','Wind_Speed(mph)','Weather_TimestampMonth','Weather_TimestampYear','Weather_TimestampHour','Weather_TimestampWeekday']
categorical_columns=['Side','State','Timezone','Wind_Direction','Weather_Condition','Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight']

numeric_transformer=Pipeline(steps=[
    ('standardize',StandardScaler()),
    ('imputer',SimpleImputer(strategy='mean'))
])
categorical_transformer=Pipeline(steps=[
    ('imputer',SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(drop='first',handle_unknown='ignore'))
])
preprocessor=ColumnTransformer(
    transformers=[
        ('num',numeric_transformer,numeric_columns),
        ('cat',categorical_transformer,categorical_columns)
    ],remainder='passthrough')

output_data=data[['End_Lat','End_Lng']]
del data['End_Lat']
del data['End_Lng']
input_data=data

location_pipeline=Pipeline([
    ('datetime_feature_transformer_weather',DateTimeFeatureTransformer('Weather_Timestamp')),
    ('datetime_feature_transformer_end_time',DateTimeFeatureTransformer('End_Time')),
    ('datetime_feature_transformer_start_time',DateTimeFeatureTransformer('Start_Time')),
    ('drop_columns',DropColumnsTransformer(columns_to_drop)),
    ('preprocessor',preprocessor),
])

input_data=location_pipeline.fit_transform(input_data)

#Visualize pipeline
set_config(display='diagram')
location_pipeline

with open('location_model','rb') as f:
    location_model=pickle.load(f)

y_predicted=location_model.predict(x_test)

print(y_predicted)

with open('location_pipeline','wb') as f:
  pickle.dump(location_pipeline,f)

columns=['Distance(mi)','Temperature(F)','Visibility(mi)','Pressure(in)','Wind_Speed(mph)','Year','Weather_TimestampMonth','Weather_TimestampYear','Weather_TimestampHour','Weather_TimestampWeekday']
scaler=StandardScaler()
scaler.fit(x_train[columns])
x_train[columns]=(scaler.transform(x_train[columns]))
x_validation[columns]=(scaler.transform(x_validation[columns]))
x_test[columns]=(scaler.transform(x_test[columns]))

x_train=tensorflow.convert_to_tensor(x_train,dtype=tensorflow.float32)
x_validation=tensorflow.convert_to_tensor(x_validation,dtype=tensorflow.float32)
x_test=tensorflow.convert_to_tensor(x_test,dtype=tensorflow.float32)

print(x_train.shape)

def build_model(hp):
    model=Sequential()
    model.add(Dense(units=hp.Int('input_units',min_value=32,max_value=512,step=32),
                           activation=hp.Choice('input_activation', values=['relu', 'tanh', 'sigmoid']),
                           input_shape=(len(input_data.columns),)))
    for i in range(hp.Int('num_layers',min_value=1,max_value=10)):
        model.add(Dense(units=hp.Int(f'layer_{i}_units', min_value=32, max_value=512, step=32),
                               activation=hp.Choice(f'layer_{i}_activation', values=['relu', 'tanh', 'sigmoid']),
                               kernel_regularizer=hp.Choice(f'layer_{i}_kernel_regularizer', values=['l1', 'l2']),
                               bias_regularizer=hp.Choice(f'layer_{i}_bias_regularizer', values=['l1', 'l2'])))
        model.add(Dropout(hp.Float(f'layer_{i}_dropout', min_value=0.0, max_value=0.9, step=0.1)))
        if hp.Boolean(f'layer_{i}_batch_normalization'):
            model.add(BatchNormalization())
    model.add(Dense(units=2,activation='linear'))
    model.compile(optimizer=hp.Choice('optimizer',values=['adam','rmsprop', 'sgd']),
                  loss='mean_squared_error',
                  metrics=['mse'])

    return model

tuner = kt.RandomSearch(
    build_model,
    objective='val_loss',
    max_trials=5,
    directory='My_final_directory_3000',
    project_name='my_project'
)
early_stopping=keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

tuner.search(x=x_train,y=y_train,
             epochs=5,
             validation_data=(x_validation,y_validation),
             callbacks=[early_stopping]
        )

best_hp=tuner.get_best_hyperparameters()[0]
location_model=build_model(best_hp)

location_model.fit(x_train,y_train,epochs=100,initial_epoch=6,validation_data=(x_validation,y_validation),callbacks=early_stopping)

import pickle 
with open('location_model','wb') as f:
  pickle.dump(location_model,f)

y_predicted=location_model.predict(x_test)

print(y_predicted)

print(y_test)

mse=tensorflow.keras.losses.mean_squared_error(y_test,y_predicted)

print(f'Testing Mean Squared Error is {np.array(mse).mean()}')

data=pd.read_csv('/content/drive/MyDrive/major_project_data.csv',nrows=5e5)

class DropColumnsTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, columns_to_drop):
        self.columns_to_drop = columns_to_drop
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        return X.drop(self.columns_to_drop, axis=1)

class DateTimeFeatureTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, column_name):
        self.column_name = column_name

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        X[self.column_name] = pd.to_datetime(X[self.column_name])
        X[str(self.column_name)+'Month'] = X[self.column_name].dt.month
        X[str(self.column_name)+'Year'] = X[self.column_name].dt.year
        X[str(self.column_name)+'Hour'] = X[self.column_name].dt.hour
        X[str(self.column_name)+'Weekday'] = X[self.column_name].dt.weekday
        del X[self.column_name]
        return X

columns_to_drop=['Bump','Roundabout','Turning_Loop','Country','Number', 'Visibility(mi)', 'Amenity', 'Bump', 'No_Exit', 'Railway', 'Roundabout', 'Stop', 'Traffic_Calming','Wind_Chill(F)','Start_Lat','Start_Lng','Description','Street',
'City','Zipcode','Airport_Code','ID','County','Start_TimeMonth','Start_TimeYear','Start_TimeHour','Start_TimeWeekday']

MinMax_columns=['End_Lat','End_Lng','Humidity(%)','End_TimeMonth','End_TimeYear','End_TimeHour','End_TimeWeekday','Weather_TimestampMonth','Weather_TimestampYear']
Robust_columns=['Distance(mi)','Temperature(F)','Pressure(in)','Precipitation(in)','Wind_Speed(mph)']

minmax_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', MinMaxScaler())
])

robust_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', RobustScaler())
])
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))
])
preprocessor = ColumnTransformer(
    transformers=[
        ('num_minmax', minmax_transformer, MinMax_columns),
        ('num_robust', robust_transformer, Robust_columns),
        ('cat', categorical_transformer, categorical_columns)
    ],
    remainder='passthrough'
)

output_data=data['Severity']
del data['Severity']
input_data=data

severity_pipeline=Pipeline([
    ('datetime_feature_transformer_weather',DateTimeFeatureTransformer('Weather_Timestamp')),
    ('datetime_feature_transformer_end_time',DateTimeFeatureTransformer('End_Time')),
    ('datetime_feature_transformer_start_time',DateTimeFeatureTransformer('Start_Time')),
    ('drop_columns',DropColumnsTransformer(columns_to_drop)),
    ('preprocessor',preprocessor),
])

input_data=severity_pipeline.fit_transform(input_data)

set_config(display='diagram')
severity_pipeline

x_train,x_test,y_train,y_test=train_test_split(input_data,output_data,train_size=0.75,random_state=0)
x_train,x_validation,y_train,y_validation=train_test_split(x_train,y_train,train_size=60/75,random_state=1)

x_train=x_train.toarray()
x_validation=x_validation.toarray()
x_test=x_test.toarray()

y_train=np.array(pd.get_dummies(y_train))
y_validation=np.array(pd.get_dummies(y_validation))
y_test=np.array(pd.get_dummies(y_test))

x_train=tensorflow.convert_to_tensor(x_train,dtype=tensorflow.float32)
x_validation=tensorflow.convert_to_tensor(x_validation,dtype=tensorflow.float32)
x_test=tensorflow.convert_to_tensor(x_test,dtype=tensorflow.float32)

with open('severity_model','rb') as f:
    severity_model=pickle.load(f)

from joblib import dump
dump(severity_model,'/content/severity_model.joblib')

y_predicted=np.argmax(severity_model.predict(x_test),axis=1)
y_test=np.argmax(y_test,axis=1)

with open('severity_pipeline','wb') as f:
  pickle.dump(severity_pipeline,f)

data=pd.read_csv('/content/drive/MyDrive/major_project_data.csv',nrows=5e5)

#Location pipeline
class DropColumnsTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, columns_to_drop):
        self.columns_to_drop = columns_to_drop
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        return X.drop(self.columns_to_drop, axis=1)

class DateTimeFeatureTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, column_name):
        self.column_name = column_name

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        X[self.column_name] = pd.to_datetime(X[self.column_name])
        X[str(self.column_name)+'Month'] = X[self.column_name].dt.month
        X[str(self.column_name)+'Year'] = X[self.column_name].dt.year
        X[str(self.column_name)+'Hour'] = X[self.column_name].dt.hour
        X[str(self.column_name)+'Weekday'] = X[self.column_name].dt.weekday
        del X[self.column_name]
        return X

columns_to_drop=['Country','Turning_Loop','Roundabout','Bump','Humidity(%)','Precipitation(in)', 'Amenity', 'Give_Way', 'Railway', 'Traffic_Calming','Wind_Chill(F)', 'No_Exit', 'End_TimeMonth', 'End_TimeHour', 'End_TimeWeekday','ID','Airport_Code','Description','Street','City','Zipcode','County','Start_TimeYear','Start_TimeHour','Start_TimeWeekday','Start_TimeMonth','Number']

numeric_columns=['Distance(mi)','Temperature(F)','Visibility(mi)','End_TimeYear','Pressure(in)','Wind_Speed(mph)','Weather_TimestampMonth','Weather_TimestampYear','Weather_TimestampHour','Weather_TimestampWeekday']
categorical_columns=['Side','State','Timezone','Wind_Direction','Weather_Condition','Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight']

numeric_transformer=Pipeline(steps=[
    ('standardize',StandardScaler()),
    ('imputer',SimpleImputer(strategy='mean'))
])
categorical_transformer=Pipeline(steps=[
    ('imputer',SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(drop='first',handle_unknown='ignore'))
])
preprocessor=ColumnTransformer(
    transformers=[
        ('num',numeric_transformer,numeric_columns),
        ('cat',categorical_transformer,categorical_columns)
    ],remainder='passthrough')

output_data=data[['End_Lat','End_Lng']]
del data['End_Lat']
del data['End_Lng']
input_data=data

location_pipeline=Pipeline([
    ('datetime_feature_transformer_weather',DateTimeFeatureTransformer('Weather_Timestamp')),
    ('datetime_feature_transformer_end_time',DateTimeFeatureTransformer('End_Time')),
    ('datetime_feature_transformer_start_time',DateTimeFeatureTransformer('Start_Time')),
    ('drop_columns',DropColumnsTransformer(columns_to_drop)),
    ('preprocessor',preprocessor),
])

input_data=location_pipeline.fit_transform(input_data)

#Visualize pipeline
set_config(display='diagram')
location_pipeline

with open('location_model','rb') as f:
    location_model=pickle.load(f)

from joblib import dump
dump(location_model,'/content/location_model.joblib')

y_predicted=location_model.predict(x_test)

print(y_predicted)

with open('location_pipeline','wb') as f:
  pickle.dump(location_pipeline,f)

data=pd.read_csv('/content/drive/MyDrive/major_project_data.csv',nrows=5e5)

#Now applying feature extraction technique
#PCA
from sklearn.decomposition import PCA
pca=PCA(n_components=0.99)
columns_to_drop=['Bump','Roundabout','Turning_Loop','Country','Number', 'Visibility(mi)', 'Amenity', 'Bump', 'No_Exit', 'Railway', 'Roundabout', 'Stop', 'Traffic_Calming','Wind_Chill(F)','Start_Lat','Start_Lng','Description','Street',
'City','Zipcode','Airport_Code','ID','County','Start_TimeMonth','Start_TimeYear','Start_TimeHour','Start_TimeWeekday','End_TimeMonth','End_TimeYear','End_TimeWeekday']
MinMax_columns=['End_Lat','End_Lng','Humidity(%)','Weather_TimestampWeekday','Weather_TimestampHour','Weather_TimestampMonth','Weather_TimestampYear']
Robust_columns=['Distance(mi)','Temperature(F)','Pressure(in)','Precipitation(in)','Wind_Speed(mph)']
categorical_columns=['Side','State','Timezone','Wind_Direction','Weather_Condition','Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight']

minmax_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', MinMaxScaler())
])

robust_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', RobustScaler())
])
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))
])
preprocessor = ColumnTransformer(
    transformers=[
        ('num_minmax', minmax_transformer, MinMax_columns),
        ('num_robust', robust_transformer, Robust_columns),
        ('cat', categorical_transformer, categorical_columns)
    ],
    remainder='passthrough'
)

output_data=data['Severity']
del data['Severity']
input_data=data

from sklearn.preprocessing import FunctionTransformer
dense_transformer = FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)

severity_pipeline=Pipeline([
    ('datetime_feature_transformer_weather',DateTimeFeatureTransformer('Weather_Timestamp')),
    ('datetime_feature_transformer_end_time',DateTimeFeatureTransformer('End_Time')),
    ('datetime_feature_transformer_start_time',DateTimeFeatureTransformer('Start_Time')),
    ('drop_columns',DropColumnsTransformer(columns_to_drop)),
    ('preprocessor',preprocessor),
    ('dense_transformer',dense_transformer),
    ('pca',pca)
])

input_data=severity_pipeline.fit_transform(input_data)

set_config(display='diagram')
severity_pipeline

print(input_data.shape)

x_train,x_test,y_train,y_test=train_test_split(input_data,output_data,train_size=0.75,random_state=0)
x_train,x_validation,y_train,y_validation=train_test_split(x_train,y_train,train_size=60/75,random_state=1)
y_train=np.array(pd.get_dummies(y_train))
y_validation=np.array(pd.get_dummies(y_validation))
y_test=np.array(pd.get_dummies(y_test))
x_train=tensorflow.convert_to_tensor(x_train,dtype=tensorflow.float32)
x_validation=tensorflow.convert_to_tensor(x_validation,dtype=tensorflow.float32)
x_test=tensorflow.convert_to_tensor(x_test,dtype=tensorflow.float32)
import os
os.environ['TF_KERAS_TUNER_FLAG'] = '1'
def build_model(hp):
    model=Sequential()
    model.add(Dense(units=hp.Int('input_units',min_value=32,max_value=512,step=32),
                           activation=hp.Choice('input_activation', values=['relu', 'tanh', 'sigmoid']),
                           input_shape=((x_train.shape[1]),)))
    for i in range(hp.Int('num_layers',min_value=1,max_value=10)):
        model.add(Dense(units=hp.Int(f'layer_{i}_units', min_value=32, max_value=512, step=32),
                               activation=hp.Choice(f'layer_{i}_activation', values=['relu', 'tanh', 'sigmoid']),
                               kernel_regularizer=hp.Choice(f'layer_{i}_kernel_regularizer', values=['l1', 'l2']),
                               bias_regularizer=hp.Choice(f'layer_{i}_bias_regularizer', values=['l1', 'l2'])))
        model.add(Dropout(hp.Float(f'layer_{i}_dropout', min_value=0.0, max_value=0.9, step=0.1)))
        if hp.Boolean(f'layer_{i}_batch_normalization'):
            model.add(BatchNormalization())
    model.add(Dense(units=3, activation='softmax'))
    model.compile(optimizer=hp.Choice('optimizer',values=['adam','rmsprop', 'sgd']),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model
tuner = kt.RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    directory='My_Final_Directory_2023',
    project_name='my_project'
)
early_stopping=keras.callbacks.EarlyStopping(
    monitor='val_accuracy',
    patience=3,
    restore_best_weights=True
)
tuner.search(x=x_train,y=y_train,
             epochs=5,
             validation_data=(x_validation,y_validation),
             callbacks=[early_stopping]
        )

best_hp=tuner.get_best_hyperparameters()[0]
severity_model=build_model(best_hp)
severity_model.fit(x_train,y_train,epochs=100,initial_epoch=6,validation_data=(x_validation,y_validation),callbacks=early_stopping)

import pickle 
with open('final_severity_model','wb') as f:
  pickle.dump(severity_model,f)

y_predicted=np.argmax(severity_model.predict(x_test),axis=1)

output_map={0:'Medium Severity',1:'High Severity',2:'Very Severe'}

print(Counter((y_predicted)))

print(Counter(np.argmax(y_test,axis=1)))

"""## more Exploratory Data Analysis"""

data=pd.read_csv('/content/drive/MyDrive/major_project/major_project_data.csv',nrows=5e5)

data.columns

data_sev = data.groupby('Severity').size()
data_sev =  data_sev[[2,3,4]]
cols = ['bisque', 'rosybrown', 'palegoldenrod']
cases = ['Severity 2','Severity 3','Severity 4']
plt.figure(figsize=(10,6))
plt.pie(data_sev,
        colors = cols,
        labels= cases,
        explode = (0,0,0.2), #moving slices apart
        autopct = ('%1.1f%%')) #to display %
plt.title('Types \nof Severity', weight='heavy', fontsize=22, style='italic');

pair = sns.pairplot(data[['Severity','Temperature(F)','Humidity(%)','Pressure(in)']].dropna(), hue='Severity', palette='nipy_spectral')
# pair = sns.pairplot(df[['Severity','Temperature(F)']].dropna(), hue='Severity', palette='nipy_spectral')

pair.fig.suptitle('Distribution of Temp , Humidity and Pressure with Severity', y =1.08 
                  , fontsize = 16 , color = 'MidnightBlue' , ha = 'center' , va='top')

plt.show()

data.Start_Time=pd.to_datetime(data.Start_Time)
data.End_Time=pd.to_datetime(data.End_Time)

Month_day=pd.crosstab(data['Start_Time'].dt.day, data['End_Time'].dt.month)

ax=sns.heatmap(Month_day,linewidths=.5, cmap='coolwarm')
ax.set_title("USA accidents (Month-Day)")
ax.set(xlabel='Month', ylabel='Day')
plt.tight_layout()

Week_day=pd.crosstab(data['Start_Time'].dt.hour, data['End_Time'].dt.dayofweek+1)
ax1=sns.heatmap(Week_day,linewidths=.5, cmap='coolwarm',)
ax1.set_title("USA accidents (Weekday - Hour)")
ax1.set(xlabel='Day of the week', ylabel='Hour')
plt.tight_layout()

period_features = ['Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight']
fig, axs = plt.subplots(ncols=1, nrows=4, figsize=(13, 5))

plt.subplots_adjust(wspace = 0.5)
for i, feature in enumerate(period_features, 1):    
    plt.subplot(1, 4, i)
    sns.countplot(x=feature, hue='Severity', data=data ,palette="Set2")
    
    plt.xlabel('{}'.format(feature), size=12, labelpad=3)
    plt.ylabel('Accident Count', size=12, labelpad=3)    
    plt.tick_params(axis='x', labelsize=12)
    plt.tick_params(axis='y', labelsize=12)
    
    plt.legend(['2', '3','4'], loc='upper right', prop={'size': 10})
    plt.title('Count of Severity in\n{} Feature'.format(feature), size=13, y=1.05)
fig.suptitle('Count of Accidents by Period-of-Day (resampled data)',y=1.08, fontsize=16)
plt.show()

factors = ['Temperature(F)','Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)']

for factor in factors:
    # remove some of the extreme values
    factorMin = data[factor].quantile(q=0.0001)
    factorMax = data[factor].quantile(q=0.9999)
    # print df["Severity"].groupby(pd.cut(df[factor], np.linspace(factorMin,factorMax,num=20))).count()
    plt.subplots(figsize=(15,5))
    for s in np.arange(1,5):
        data["Severity"].groupby(pd.cut(data[factor], np.linspace(factorMin,factorMax,num=20))).mean().plot()
        plt.title("Mean Severity as a Function of " + factor, fontsize=16)
        plt.xlabel(factor + " Range", fontsize=16)
        plt.ylabel("Mean Severity", fontsize=16)
        plt.xticks(fontsize=11)
        plt.yticks(fontsize=16)

sns.scatterplot(y=data.Start_Lat,x=data.Start_Lng,hue=data["Temperature(F)"])
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.title("Temperature(F) heatmap in US ")
plt.legend()

sns.scatterplot(y=data.Start_Lat,x=data.Start_Lng,hue=data["Wind_Chill(F)"])
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.title("Wind_Chill(F) heatmap in US ")
plt.legend()

sns.scatterplot(y=data.Start_Lat,x=data.Start_Lng,hue=data["Humidity(%)"])
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.title("Humidity(%) heatmap in US ")
plt.legend()